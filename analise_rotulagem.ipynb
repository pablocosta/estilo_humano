{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_sino_neg = pd.read_csv(\"./data/clustered/cinovac_neg.csv\", sep=\";\")\n",
    "df_sino_pos = pd.read_csv(\"./data/clustered/cinovac_pos.csv\", sep=\";\")\n",
    "df_cloroquina_neg = pd.read_csv(\"./data/clustered/cloroquina_neg.csv\", sep=\";\")\n",
    "df_cloroquina_pos = pd.read_csv(\"./data/clustered/cloroquina_pos.csv\", sep=\";\")\n",
    "df_igreja_neg = pd.read_csv(\"./data/clustered/igreja_neg.csv\", sep=\";\")\n",
    "df_igreja_pos = pd.read_csv(\"./data/clustered/igreja_pos.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "(4, 8)\n",
      "(3, 8)\n",
      "(8, 8)\n",
      "(0, 8)\n",
      "(6, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df_sino_neg[df_sino_neg[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1].shape)\n",
    "print(df_sino_pos[df_sino_pos[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1].shape)\n",
    "\n",
    "print(df_cloroquina_neg[df_cloroquina_neg[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1].shape)\n",
    "print(df_cloroquina_pos[df_cloroquina_pos[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1].shape)\n",
    "\n",
    "print(df_igreja_neg[df_igreja_neg[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1].shape)\n",
    "print(df_igreja_pos[df_igreja_pos[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1].shape)\n",
    "\n",
    "df_sino_neg = df_sino_neg[df_sino_neg[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1]\n",
    "df_sino_pos = df_sino_pos[df_sino_pos[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1]\n",
    "\n",
    "df_cloroquina_neg = df_cloroquina_neg[df_cloroquina_neg[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1]\n",
    "df_cloroquina_pos = df_cloroquina_pos[df_cloroquina_pos[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1]\n",
    "\n",
    "df_igreja_neg = df_igreja_neg[df_igreja_neg[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1]\n",
    "df_igreja_pos = df_igreja_pos[df_igreja_pos[\"Voce considera que o segundo texto é uma reecrita do primeiro?1 - Sim2 - Não \"] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "def tokenize(text, nGram=3):\n",
    "    text = text.split()\n",
    "    grams = []\n",
    "    for i in range(1, nGram):\n",
    "        i_grams = [\n",
    "            \" \".join(gram)\n",
    "            for gram in ngrams(text, i)\n",
    "        ]\n",
    "        grams.extend(i_grams)\n",
    "        \n",
    "    return grams\n",
    "\n",
    "def getNgramOverlap(hypothesys, references, nGram):\n",
    "\n",
    "  overlaps = []\n",
    "  for h, r in zip(hypothesys, references):\n",
    "    if (h.strip() == \"\") or (r.strip() == \"\"):\n",
    "      overlaps.append(1.0)\n",
    "      continue\n",
    "    a = tokenize(h, nGram)\n",
    "    b = tokenize(r, nGram)\n",
    "    if len(a) >= len(b):\n",
    "      overlaps.append(len(set(a) & set(b))/len(a))\n",
    "    elif len(b) >= len(a):\n",
    "      overlaps.append(len(set(a) & set(b))/len(b))\n",
    "\n",
    "  return overlaps\n",
    "\n",
    "\n",
    "def taxa_sobreposicao_pronomes(lista1, lista2):\n",
    "    set1 = set(lista1)\n",
    "    set2 = set(lista2)\n",
    "\n",
    "    # Calcula a interseção e a união dos conjuntos\n",
    "    intersecao = set1.intersection(set2)\n",
    "    uniao = set1.union(set2)\n",
    "\n",
    "    # Calcula a taxa de sobreposição (Jaccard)\n",
    "    if len(uniao) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(intersecao) / len(uniao)\n",
    "    \n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "tqdm.pandas()\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "def get_pos(x):\n",
    "    pos = []\n",
    "    for s in x:\n",
    "        s_pos = []\n",
    "        for w in nlp(s):\n",
    "            s_pos.append(w.pos_)\n",
    "        pos.append(s_pos)\n",
    "    return pos\n",
    "\n",
    "\n",
    "def get_avg_std(lista, busca):\n",
    "    ocorrencias = []\n",
    "    \n",
    "    for sentenca in lista:\n",
    "        ocorrencias.append(sentenca.count(busca))\n",
    "    if len(ocorrencias) == 0:\n",
    "        return 0.0, 0.0\n",
    "    else:\n",
    "        return statistics.mean(ocorrencias), statistics.stdev(ocorrencias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11970551378446115\n",
      "0.08173076923076923\n",
      "=====================================\n",
      "0.12314814814814816\n",
      "0.04834620716973659\n",
      "=====================================\n",
      "0.0\n",
      "0.19259259259259257\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calcula a taxa de sobreposição de pronomes entre duas sentenças\n",
    "\"\"\"\n",
    "import ast\n",
    "taxas = []\n",
    "\n",
    "for line in df_sino_neg.iterrows():\n",
    "\n",
    "    taxas.append(taxa_sobreposicao_pronomes(ast.literal_eval(line[1][\"nouns_origem\"]).keys(), ast.literal_eval(line[1][\"nouns_alvo\"]).keys()))\n",
    "\n",
    "print(np.mean(taxas))\n",
    "\n",
    "taxas = []\n",
    "\n",
    "for line in df_sino_pos.iterrows():\n",
    "\n",
    "    taxas.append(taxa_sobreposicao_pronomes(ast.literal_eval(line[1][\"nouns_origem\"]).keys(), ast.literal_eval(line[1][\"nouns_alvo\"]).keys()))\n",
    "\n",
    "print(np.mean(taxas))\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "taxas = []\n",
    "\n",
    "for line in df_cloroquina_neg.iterrows():\n",
    "\n",
    "    taxas.append(taxa_sobreposicao_pronomes(ast.literal_eval(line[1][\"nouns_origem\"]).keys(), ast.literal_eval(line[1][\"nouns_alvo\"]).keys()))\n",
    "\n",
    "print(np.mean(taxas))\n",
    "\n",
    "taxas = []\n",
    "\n",
    "for line in df_cloroquina_pos.iterrows():\n",
    "    \n",
    "    taxas.append(taxa_sobreposicao_pronomes(ast.literal_eval(line[1][\"nouns_origem\"]).keys(), ast.literal_eval(line[1][\"nouns_alvo\"]).keys()))\n",
    "\n",
    "print(np.mean(taxas))\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "taxas = []\n",
    "\n",
    "for line in df_igreja_neg.iterrows():\n",
    "    \n",
    "    taxas.append(taxa_sobreposicao_pronomes(ast.literal_eval(line[1][\"nouns_origem\"]).keys(), ast.literal_eval(line[1][\"nouns_alvo\"]).keys()))\n",
    "\n",
    "print(0.0)\n",
    "\n",
    "taxas = []\n",
    "\n",
    "for line in df_igreja_pos.iterrows():\n",
    "    \n",
    "    taxas.append(taxa_sobreposicao_pronomes(ast.literal_eval(line[1][\"nouns_origem\"]).keys(), ast.literal_eval(line[1][\"nouns_alvo\"]).keys()))\n",
    "\n",
    "print(np.mean(taxas))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05998490202728576\n",
      "0.08769113941527734\n",
      "========================================\n",
      "0.02899601304820587\n",
      "0.045931740501886476\n",
      "========================================\n",
      "0.0\n",
      "0.043877210543877214\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(getNgramOverlap(df_sino_neg[\"textos_origem\"], df_sino_neg[\"textos_alvo\"], nGram=3)))\n",
    "print(np.mean(getNgramOverlap(df_sino_pos[\"textos_origem\"], df_sino_pos[\"textos_alvo\"], nGram=3)))\n",
    "\n",
    "print(\"========================================\")\n",
    "\n",
    "print(np.mean(getNgramOverlap(df_cloroquina_neg[\"textos_origem\"], df_cloroquina_neg[\"textos_alvo\"], nGram=3)))\n",
    "print(np.mean(getNgramOverlap(df_cloroquina_pos[\"textos_origem\"], df_cloroquina_pos[\"textos_alvo\"], nGram=3)))\n",
    "\n",
    "print(\"========================================\")\n",
    "\n",
    "#print(np.mean(getNgramOverlap(df_igreja_neg[\"textos_origem\"], df_igreja_neg[\"textos_alvo\"], nGram=3))) 0.0 0.0\n",
    "print(0.0)\n",
    "print(np.mean(getNgramOverlap(df_igreja_pos[\"textos_origem\"], df_igreja_pos[\"textos_alvo\"], nGram=3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. and std. NOUN (3, 1.4770978917519928)\n",
      "avg. and std. PUNCT (0.3333333333333333, 0.49236596391733095)\n",
      "avg. and std. ADP (1.6666666666666667, 1.1547005383792515)\n",
      "avg. and std. VERB (2.5, 1.0)\n",
      "avg. and std. DET (0.5, 0.5222329678670935)\n",
      "avg. and std. PRON (1.1666666666666667, 0.7177405625652734)\n",
      "avg. and std. ADV (0.8333333333333334, 0.7177405625652734)\n",
      "avg. and std. PROPN (0.16666666666666666, 0.3892494720807615)\n",
      "avg. and std. CCONJ (0.5, 0.5222329678670935)\n",
      "avg. and std. ADJ (1, 0.6030226891555273)\n",
      "=====================================\n",
      "avg. and std. NOUN (0.0, 0.0)\n",
      "avg. and std. PUNCT (0.0, 0.0)\n",
      "avg. and std. ADP (0.0, 0.0)\n",
      "avg. and std. VERB (0.0, 0.0)\n",
      "avg. and std. DET (0.0, 0.0)\n",
      "avg. and std. PRON (0.0, 0.0)\n",
      "avg. and std. ADV (0.0, 0.0)\n",
      "avg. and std. PROPN (0.0, 0.0)\n",
      "avg. and std. CCONJ (0.0, 0.0)\n",
      "avg. and std. ADJ (0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "pos_origem_igreja = get_pos(df_igreja_pos[\"textos_origem\"].to_list()+df_igreja_pos[\"textos_alvo\"].to_list())\n",
    "\n",
    "print(\"avg. and std. NOUN\", get_avg_std(pos_origem_igreja, \"NOUN\"))\n",
    "print(\"avg. and std. PUNCT\", get_avg_std(pos_origem_igreja, \"PUNCT\"))\n",
    "print(\"avg. and std. ADP\", get_avg_std(pos_origem_igreja, \"ADP\"))\n",
    "print(\"avg. and std. VERB\", get_avg_std(pos_origem_igreja, \"VERB\"))\n",
    "print(\"avg. and std. DET\", get_avg_std(pos_origem_igreja, \"DET\"))\n",
    "print(\"avg. and std. PRON\", get_avg_std(pos_origem_igreja, \"PRON\"))\n",
    "print(\"avg. and std. ADV\", get_avg_std(pos_origem_igreja, \"ADV\"))\n",
    "print(\"avg. and std. PROPN\", get_avg_std(pos_origem_igreja, \"PROPN\"))\n",
    "print(\"avg. and std. CCONJ\", get_avg_std(pos_origem_igreja, \"CCONJ\"))\n",
    "print(\"avg. and std. ADJ\", get_avg_std(pos_origem_igreja, \"ADJ\"))\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "neg_origem_igreja = get_pos(df_igreja_neg[\"textos_origem\"].to_list()+df_igreja_neg[\"textos_alvo\"].to_list())\n",
    "\n",
    "print(\"avg. and std. NOUN\", get_avg_std(neg_origem_igreja, \"NOUN\"))\n",
    "print(\"avg. and std. PUNCT\", get_avg_std(neg_origem_igreja, \"PUNCT\"))\n",
    "print(\"avg. and std. ADP\", get_avg_std(neg_origem_igreja, \"ADP\"))\n",
    "print(\"avg. and std. VERB\", get_avg_std(neg_origem_igreja, \"VERB\"))\n",
    "print(\"avg. and std. DET\", get_avg_std(neg_origem_igreja, \"DET\"))\n",
    "print(\"avg. and std. PRON\", get_avg_std(neg_origem_igreja, \"PRON\"))\n",
    "print(\"avg. and std. ADV\", get_avg_std(neg_origem_igreja, \"ADV\"))\n",
    "print(\"avg. and std. PROPN\", get_avg_std(neg_origem_igreja, \"PROPN\"))\n",
    "print(\"avg. and std. CCONJ\", get_avg_std(neg_origem_igreja, \"CCONJ\"))\n",
    "print(\"avg. and std. ADJ\", get_avg_std(neg_origem_igreja, \"ADJ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. and std. NOUN (4.125, 3.758324094593227)\n",
      "avg. and std. PUNCT (1.375, 1.0606601717798212)\n",
      "avg. and std. ADP (3.5, 1.6903085094570331)\n",
      "avg. and std. VERB (2.5, 1.0690449676496976)\n",
      "avg. and std. DET (2.75, 1.3887301496588271)\n",
      "avg. and std. PRON (1.125, 0.8345229603962802)\n",
      "avg. and std. ADV (0.875, 0.6408699444616558)\n",
      "avg. and std. PROPN (2.375, 1.1877349391654208)\n",
      "avg. and std. CCONJ (0.375, 0.7440238091428449)\n",
      "avg. and std. ADJ (1.25, 1.5811388300841898)\n",
      "=====================================\n",
      "avg. and std. NOUN (4.75, 2.9640705601780613)\n",
      "avg. and std. PUNCT (3.875, 1.807721533549109)\n",
      "avg. and std. ADP (4.625, 1.9226098333849673)\n",
      "avg. and std. VERB (5.25, 1.8322507626258087)\n",
      "avg. and std. DET (2.75, 1.0350983390135313)\n",
      "avg. and std. PRON (1.25, 0.8864052604279183)\n",
      "avg. and std. ADV (2.875, 1.552647508520297)\n",
      "avg. and std. PROPN (3, 2.563479777846623)\n",
      "avg. and std. CCONJ (1, 0.9258200997725514)\n",
      "avg. and std. ADJ (2, 1.4142135623730951)\n"
     ]
    }
   ],
   "source": [
    "pos_origem_sino = get_pos(df_sino_pos[\"textos_origem\"].to_list()+df_sino_pos[\"textos_alvo\"].to_list())\n",
    "\n",
    "print(\"avg. and std. NOUN\", get_avg_std(pos_origem_sino, \"NOUN\"))\n",
    "print(\"avg. and std. PUNCT\", get_avg_std(pos_origem_sino, \"PUNCT\"))\n",
    "print(\"avg. and std. ADP\", get_avg_std(pos_origem_sino, \"ADP\"))\n",
    "print(\"avg. and std. VERB\", get_avg_std(pos_origem_sino, \"VERB\"))\n",
    "print(\"avg. and std. DET\", get_avg_std(pos_origem_sino, \"DET\"))\n",
    "print(\"avg. and std. PRON\", get_avg_std(pos_origem_sino, \"PRON\"))\n",
    "print(\"avg. and std. ADV\", get_avg_std(pos_origem_sino, \"ADV\"))\n",
    "print(\"avg. and std. PROPN\", get_avg_std(pos_origem_sino, \"PROPN\"))\n",
    "print(\"avg. and std. CCONJ\", get_avg_std(pos_origem_sino, \"CCONJ\"))\n",
    "print(\"avg. and std. ADJ\", get_avg_std(pos_origem_sino, \"ADJ\"))\n",
    "\n",
    "print(\"=====================================\")\n",
    "neg_origem_sino = get_pos(df_sino_neg[\"textos_origem\"].to_list()+df_sino_neg[\"textos_alvo\"].to_list())\n",
    "\n",
    "print(\"avg. and std. NOUN\", get_avg_std(neg_origem_sino, \"NOUN\"))\n",
    "print(\"avg. and std. PUNCT\", get_avg_std(neg_origem_sino, \"PUNCT\"))\n",
    "print(\"avg. and std. ADP\", get_avg_std(neg_origem_sino, \"ADP\"))\n",
    "print(\"avg. and std. VERB\", get_avg_std(neg_origem_sino, \"VERB\"))\n",
    "print(\"avg. and std. DET\", get_avg_std(neg_origem_sino, \"DET\"))\n",
    "print(\"avg. and std. PRON\", get_avg_std(neg_origem_sino, \"PRON\"))\n",
    "print(\"avg. and std. ADV\", get_avg_std(neg_origem_sino, \"ADV\"))\n",
    "print(\"avg. and std. PROPN\", get_avg_std(neg_origem_sino, \"PROPN\"))\n",
    "print(\"avg. and std. CCONJ\", get_avg_std(neg_origem_sino, \"CCONJ\"))\n",
    "print(\"avg. and std. ADJ\", get_avg_std(neg_origem_sino, \"ADJ\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. and std. NOUN (6.5625, 3.982775414205526)\n",
      "avg. and std. PUNCT (4.8125, 2.372586492979058)\n",
      "avg. and std. ADP (3.625, 1.857417562100671)\n",
      "avg. and std. VERB (5.4375, 3.0977142110487423)\n",
      "avg. and std. DET (3.875, 3.263433774416144)\n",
      "avg. and std. PRON (1.8125, 1.4244882121894398)\n",
      "avg. and std. ADV (2.3125, 2.2721135535003527)\n",
      "avg. and std. PROPN (2.9375, 3.2958306995353994)\n",
      "avg. and std. CCONJ (1.4375, 0.963932916061417)\n",
      "avg. and std. ADJ (1.125, 1.1474609652039003)\n",
      "=====================================\n",
      "avg. and std. NOUN (4.333333333333333, 3.32665998663324)\n",
      "avg. and std. PUNCT (1.8333333333333333, 1.7224014243685084)\n",
      "avg. and std. ADP (3.3333333333333335, 3.444802848737017)\n",
      "avg. and std. VERB (3.3333333333333335, 1.2110601416389966)\n",
      "avg. and std. DET (1.3333333333333333, 1.632993161855452)\n",
      "avg. and std. PRON (2, 2.0)\n",
      "avg. and std. ADV (0.6666666666666666, 0.5163977794943223)\n",
      "avg. and std. PROPN (0.8333333333333334, 1.602081978759722)\n",
      "avg. and std. CCONJ (0.8333333333333334, 1.1690451944500122)\n",
      "avg. and std. ADJ (1.5, 1.760681686165901)\n"
     ]
    }
   ],
   "source": [
    "pos_origem_cloroquina = get_pos(df_cloroquina_pos[\"textos_origem\"].to_list()+df_cloroquina_pos[\"textos_alvo\"].to_list())\n",
    "\n",
    "print(\"avg. and std. NOUN\", get_avg_std(pos_origem_cloroquina, \"NOUN\"))\n",
    "print(\"avg. and std. PUNCT\", get_avg_std(pos_origem_cloroquina, \"PUNCT\"))\n",
    "print(\"avg. and std. ADP\", get_avg_std(pos_origem_cloroquina, \"ADP\"))\n",
    "print(\"avg. and std. VERB\", get_avg_std(pos_origem_cloroquina, \"VERB\"))\n",
    "print(\"avg. and std. DET\", get_avg_std(pos_origem_cloroquina, \"DET\"))\n",
    "print(\"avg. and std. PRON\", get_avg_std(pos_origem_cloroquina, \"PRON\"))\n",
    "print(\"avg. and std. ADV\", get_avg_std(pos_origem_cloroquina, \"ADV\"))\n",
    "print(\"avg. and std. PROPN\", get_avg_std(pos_origem_cloroquina, \"PROPN\"))\n",
    "print(\"avg. and std. CCONJ\", get_avg_std(pos_origem_cloroquina, \"CCONJ\"))\n",
    "print(\"avg. and std. ADJ\", get_avg_std(pos_origem_cloroquina, \"ADJ\"))\n",
    "\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "neg_origem_cloroquina = get_pos(df_cloroquina_neg[\"textos_origem\"].to_list()+df_cloroquina_neg[\"textos_alvo\"].to_list())\n",
    "\n",
    "print(\"avg. and std. NOUN\", get_avg_std(neg_origem_cloroquina, \"NOUN\"))\n",
    "print(\"avg. and std. PUNCT\", get_avg_std(neg_origem_cloroquina, \"PUNCT\"))\n",
    "print(\"avg. and std. ADP\", get_avg_std(neg_origem_cloroquina, \"ADP\"))\n",
    "print(\"avg. and std. VERB\", get_avg_std(neg_origem_cloroquina, \"VERB\"))\n",
    "print(\"avg. and std. DET\", get_avg_std(neg_origem_cloroquina, \"DET\"))\n",
    "print(\"avg. and std. PRON\", get_avg_std(neg_origem_cloroquina, \"PRON\"))\n",
    "print(\"avg. and std. ADV\", get_avg_std(neg_origem_cloroquina, \"ADV\"))\n",
    "print(\"avg. and std. PROPN\", get_avg_std(neg_origem_cloroquina, \"PROPN\"))\n",
    "print(\"avg. and std. CCONJ\", get_avg_std(neg_origem_cloroquina, \"CCONJ\"))\n",
    "print(\"avg. and std. ADJ\", get_avg_std(neg_origem_cloroquina, \"ADJ\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Função que calcula a correlação de Kendall (tau) entre duas listas de textos\n",
    "def correlacao_tau_textos(lista1, lista2):\n",
    "    # Combina as duas listas\n",
    "    textos_completos = lista1 + lista2\n",
    "\n",
    "    # Converte os textos para representações vetoriais (bag-of-words)\n",
    "    vectorizer = CountVectorizer()\n",
    "    matriz_vetores = vectorizer.fit_transform(textos_completos).toarray()\n",
    "\n",
    "    # Divide novamente as matrizes\n",
    "    vetores_lista1 = matriz_vetores[:len(lista1)].sum(axis=1)\n",
    "    vetores_lista2 = matriz_vetores[len(lista1):].sum(axis=1)\n",
    "\n",
    "    # Calcula a correlação tau de Kendall\n",
    "    tau, p_valor = kendalltau(vetores_lista1, vetores_lista2)\n",
    "\n",
    "    return tau, p_valor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlação tau: 0.816496580927726\n",
      "P-valor: 0.22067136191984693\n",
      "Correlação tau: 0.32732683535398854\n",
      "P-valor: 0.2618277009271762\n",
      "=====================================\n",
      "Correlação tau: 0.0\n",
      "P-valor: 1.0\n",
      "Correlação tau: 0.7745966692414834\n",
      "P-valor: 0.15729920705028502\n",
      "=====================================\n",
      "Correlação tau: 0.0\n",
      "P-valor: 0.0\n",
      "Correlação tau: 0.07692307692307693\n",
      "P-valor: 0.8402142310487369\n"
     ]
    }
   ],
   "source": [
    "# Calcula a correlação de tau entre os textos\n",
    "\n",
    "lista1 = df_cloroquina_neg[\"textos_origem\"].to_list()\n",
    "lista2 = df_cloroquina_neg[\"textos_alvo\"].to_list()\n",
    "\n",
    "tau, p_valor = correlacao_tau_textos(lista1, lista2)\n",
    "print(f\"Correlação tau: {tau}\\nP-valor: {p_valor}\")\n",
    "\n",
    "lista1 = df_cloroquina_pos[\"textos_origem\"].to_list()\n",
    "lista2 = df_cloroquina_pos[\"textos_alvo\"].to_list()\n",
    "\n",
    "tau, p_valor = correlacao_tau_textos(lista1, lista2)\n",
    "\n",
    "print(f\"Correlação tau: {tau}\\nP-valor: {p_valor}\")\n",
    "\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "lista1 = df_sino_neg[\"textos_origem\"].to_list()\n",
    "lista2 = df_sino_neg[\"textos_alvo\"].to_list()\n",
    "\n",
    "tau, p_valor = correlacao_tau_textos(lista1, lista2)\n",
    "\n",
    "print(f\"Correlação tau: {tau}\\nP-valor: {p_valor}\")\n",
    "\n",
    "lista1 = df_sino_pos[\"textos_origem\"].to_list()\n",
    "lista2 = df_sino_pos[\"textos_alvo\"].to_list()\n",
    "\n",
    "tau, p_valor = correlacao_tau_textos(lista1, lista2)\n",
    "\n",
    "print(f\"Correlação tau: {tau}\\nP-valor: {p_valor}\")\n",
    "\n",
    "print(\"=====================================\")\n",
    "\n",
    "lista1 = df_igreja_neg[\"textos_origem\"].to_list()\n",
    "lista2 = df_igreja_neg[\"textos_alvo\"].to_list()\n",
    "\n",
    "tau, p_valor = 0.0 , 0.0\n",
    "\n",
    "print(f\"Correlação tau: {tau}\\nP-valor: {p_valor}\")\n",
    "\n",
    "lista1 = df_igreja_pos[\"textos_origem\"].to_list()\n",
    "lista2 = df_igreja_pos[\"textos_alvo\"].to_list()\n",
    "\n",
    "tau, p_valor = correlacao_tau_textos(lista1, lista2)\n",
    "\n",
    "print(f\"Correlação tau: {tau}\\nP-valor: {p_valor}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
